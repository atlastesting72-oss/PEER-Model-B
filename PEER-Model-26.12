import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from datasets import load_dataset
import math
import os
import time

# ==========================================
# 8-BIT OPTIMIZER
# ==========================================
try:
    import bitsandbytes as bnb
    HAS_BNB = True
    print("âœ“ bitsandbytes loaded")
except ImportError:
    HAS_BNB = False
    print("âœ— bitsandbytes not found")

# ==========================================
# DRIVE SETUP
# ==========================================
try:
    from google.colab import drive
    drive.mount('/content/drive')
    CHECKPOINT_DIR = '/content/drive/MyDrive/peer_checkpoints_v4'
except ImportError:
    CHECKPOINT_DIR = './peer_checkpoints_v4'

os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ==========================================
# CONFIGURATION
# ==========================================
class PeerConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.d_model = 384
        self.n_layer = 8
        self.n_head = 6
        self.max_len = 256
        self.dropout = 0.1

        # Expert config
        self.sqrt_n = 361                         # 361
        self.num_experts = self.sqrt_n ** 2       # 130,321 experts
        self.n_retrieval_heads = 8
        self.k_active = 16                        # 128 active per token

        # === KEY OPTIMIZATION ===
        self.expert_bottleneck = 192               # Small expert dimension 192, 64
        self.d_query = self.d_model // self.n_retrieval_heads  # 48

        # Anti-collapse
        self.router_noise_std = 0.1
        self.load_balance_coef = 0.1
        self.entropy_coef = 0.01

        # Training - FAST settings
        self.batch_size = 12
        self.gradient_accumulation = 1
        self.learning_rate = 3e-4
        self.min_lr = 3e-5
        self.max_iters = 30000
        self.warmup_iters = 1000
        self.grad_clip = 1.0
        self.weight_decay = 0.1

        self.use_gradient_checkpointing = False   # Not needed now!

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.use_8bit_adam = HAS_BNB and self.device == 'cuda'

    def __repr__(self):
        return (
            f"\n{'='*60}\n"
            f"PEER Configuration (Bottleneck Optimized)\n"
            f"{'='*60}\n"
            f"Model:      d_model={self.d_model}, n_layer={self.n_layer}\n"
            f"Experts:    {self.num_experts:,} ({self.sqrt_n}Â²)\n"
            f"Bottleneck: {self.expert_bottleneck} (vs d_model={self.d_model})\n"
            f"Routing:    {self.n_retrieval_heads} heads Ã— {self.k_active} = "
            f"{self.n_retrieval_heads * self.k_active} active/token\n"
            f"{'='*60}"
        )


config = PeerConfig()
print(config)
print(f"Device: {config.device}")

# ==========================================
# DATA
# ==========================================
print("\nLoading data...")
dataset = load_dataset("roneneldan/TinyStories", split="train[:20%]")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def fast_tokenize(examples):
    out = tokenizer(examples["text"], truncation=False, add_special_tokens=False)
    return {"input_ids": [ids + [tokenizer.eos_token_id] for ids in out["input_ids"]]}

tokenized = dataset.map(fast_tokenize, batched=True, num_proc=os.cpu_count(), remove_columns=["text"])

block_size = config.max_len + 1

def group_texts(examples):
    concatenated = sum(examples["input_ids"], [])
    total_length = (len(concatenated) // block_size) * block_size
    return {"input_ids": [concatenated[i:i+block_size] for i in range(0, total_length, block_size)]}

lm_dataset = tokenized.map(group_texts, batched=True, num_proc=os.cpu_count())
lm_dataset.set_format(type="torch", columns=["input_ids"])

train_loader = DataLoader(
    lm_dataset, batch_size=config.batch_size, shuffle=True,
    num_workers=2, pin_memory=True, drop_last=True
)
print(f"Dataset: {len(lm_dataset):,} sequences")

# ==========================================
# PRODUCT KEY RETRIEVAL
# ==========================================
class ProductKeyRetrieval(nn.Module):
    def __init__(self, d_query, sqrt_n, k_active, config):
        super().__init__()
        self.k = k_active
        self.sqrt_n = sqrt_n
        self.num_experts = sqrt_n ** 2
        self.sub_dim = d_query // 2
        self.config = config

        self.c_keys = nn.Parameter(torch.empty(sqrt_n, self.sub_dim))
        self.c_prime_keys = nn.Parameter(torch.empty(sqrt_n, self.sub_dim))
        nn.init.orthogonal_(self.c_keys)
        nn.init.orthogonal_(self.c_prime_keys)

        self.query_bn = nn.BatchNorm1d(d_query, momentum=0.1, track_running_stats=True)

        self.register_buffer('expert_counts', torch.zeros(self.num_experts))
        self.register_buffer('total_tokens', torch.tensor(0.0))

    def forward(self, query):
        b, s, h, d = query.shape

        q_flat = query.reshape(b * s * h, d)
        if q_flat.size(0) > 1:
            q_norm = self.query_bn(q_flat)
        else:
            self.query_bn.eval()
            q_norm = self.query_bn(q_flat)
            if self.training:
                self.query_bn.train()
        q_norm = q_norm.view(b, s, h, d)

        q1, q2 = q_norm.chunk(2, dim=-1)

        scores1 = torch.matmul(q1, self.c_keys.t())
        scores2 = torch.matmul(q2, self.c_prime_keys.t())

        if self.training and self.config.router_noise_std > 0:
            scores1 = scores1 + torch.randn_like(scores1) * self.config.router_noise_std
            scores2 = scores2 + torch.randn_like(scores2) * self.config.router_noise_std

        k_sub = min(self.k + 4, self.sqrt_n)
        top_s1, idx1 = torch.topk(scores1, k_sub, dim=-1)
        top_s2, idx2 = torch.topk(scores2, k_sub, dim=-1)

        joint = top_s1.unsqueeze(-1) + top_s2.unsqueeze(-2)
        joint_flat = joint.view(b, s, h, -1)

        final_scores, best_idx = torch.topk(joint_flat, self.k, dim=-1)

        row = best_idx // k_sub
        col = best_idx % k_sub
        real_row = torch.gather(idx1, -1, row)
        real_col = torch.gather(idx2, -1, col)

        indices = real_row * self.sqrt_n + real_col

        aux_loss = self._aux_loss(scores1, scores2) if self.training else 0.0

        if self.training:
            with torch.no_grad():
                flat = indices.flatten().clamp(0, self.num_experts - 1)
                self.expert_counts += torch.bincount(flat, minlength=self.num_experts).float()
                self.total_tokens += indices.numel()

        return indices, final_scores, aux_loss

    def _aux_loss(self, s1, s2):
        p1 = F.softmax(s1, dim=-1).mean(dim=(0, 1, 2))
        p2 = F.softmax(s2, dim=-1).mean(dim=(0, 1, 2))
        target = 1.0 / self.sqrt_n

        balance = ((p1 - target).pow(2).sum() + (p2 - target).pow(2).sum()) * self.sqrt_n

        eps = 1e-8
        ent1 = -(p1 * (p1 + eps).log()).sum()
        ent2 = -(p2 * (p2 + eps).log()).sum()
        max_ent = math.log(self.sqrt_n)
        entropy = 1.0 - (ent1 + ent2) / (2 * max_ent)

        return self.config.load_balance_coef * balance + self.config.entropy_coef * entropy

    def get_stats(self):
        if self.total_tokens == 0:
            return {"used": 0, "total": self.num_experts, "pct": 0.0}
        used = (self.expert_counts > 0).sum().item()
        return {"used": int(used), "total": self.num_experts, "pct": 100.0 * used / self.num_experts}

    def reset_stats(self):
        self.expert_counts.zero_()
        self.total_tokens.zero_()


# ==========================================
# BOTTLENECK EXPERT POOL (KEY OPTIMIZATION)
# ==========================================
class BottleneckExpertPool(nn.Module):
    """
    Fast expert pool using bottleneck design:
    - Shared input projection: d_model â†’ bottleneck
    - Small per-expert weights: bottleneck dimension
    - Shared output projection: bottleneck â†’ d_model

    Expert tables are 6x smaller â†’ 6x faster memory access!
    """
    def __init__(self, config):
        super().__init__()
        self.num_experts = config.num_experts
        self.d_model = config.d_model
        self.bottleneck = config.expert_bottleneck  # 64

        # Shared projections (efficient - single matmul for whole batch)
        self.in_proj = nn.Linear(config.d_model, self.bottleneck, bias=False)
        self.out_proj = nn.Linear(self.bottleneck, config.d_model, bias=False)

        # Small expert-specific weights
        self.w_down = nn.Embedding(config.num_experts, self.bottleneck)
        self.w_up = nn.Embedding(config.num_experts, self.bottleneck)

        # Init
        nn.init.normal_(self.in_proj.weight, std=0.02)
        nn.init.normal_(self.out_proj.weight, std=0.02 / math.sqrt(2 * config.n_layer))
        nn.init.normal_(self.w_down.weight, std=0.02)
        nn.init.normal_(self.w_up.weight, std=0.02)

    def forward(self, x, indices, scores):
        """
        Args:
            x: (batch, seq, d_model)
            indices: (batch, seq, heads, k)
            scores: (batch, seq, heads, k)
        """
        b, s, d = x.shape
        h = indices.size(2)
        k = indices.size(3)

        # === Shared input projection (FAST: single matmul) ===
        x_small = self.in_proj(x)  # (b, s, bottleneck)

        # === Gather small expert weights ===
        flat_idx = indices.reshape(-1)
        w_down = self.w_down(flat_idx).view(b, s, h, k, self.bottleneck)
        w_up = self.w_up(flat_idx).view(b, s, h, k, self.bottleneck)

        # === Expert computation in bottleneck space ===
        # (b, s, 1, 1, bottleneck) * (b, s, h, k, bottleneck) â†’ sum â†’ (b, s, h, k)
        x_expanded = x_small.unsqueeze(2).unsqueeze(3)
        hidden = (x_expanded * w_down).sum(dim=-1)

        hidden = F.gelu(hidden)

        # Apply routing weights
        routing = F.softmax(scores, dim=-1)
        hidden = hidden * routing  # (b, s, h, k)

        # Up projection: (b, s, h, k, 1) * (b, s, h, k, bottleneck) â†’ sum â†’ (b, s, bottleneck)
        hidden = hidden.unsqueeze(-1)
        out_small = (hidden * w_up).sum(dim=(2, 3))  # (b, s, bottleneck)

        # === Shared output projection (FAST: single matmul) ===
        output = self.out_proj(out_small)  # (b, s, d_model)

        return output


# ==========================================
# PEER LAYER
# ==========================================
class PEERLayer(nn.Module):
    def __init__(self, config, expert_pool):
        super().__init__()
        self.n_heads = config.n_retrieval_heads
        self.d_query = config.d_query

        self.expert_pool = expert_pool
        self.query_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.router = ProductKeyRetrieval(self.d_query, config.sqrt_n, config.k_active, config)
        self.dropout = nn.Dropout(config.dropout)

        nn.init.normal_(self.query_proj.weight, std=0.02)

    def forward(self, x):
        b, s, d = x.shape
        queries = self.query_proj(x).view(b, s, self.n_heads, self.d_query)
        indices, scores, aux = self.router(queries)
        output = self.expert_pool(x, indices, scores)
        return self.dropout(output), aux


# ==========================================
# ATTENTION
# ==========================================
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.d_model = config.d_model
        self.head_dim = config.d_model // config.n_head

        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model, bias=False)
        self.c_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.dropout = config.dropout

        nn.init.normal_(self.c_attn.weight, std=0.02)
        nn.init.normal_(self.c_proj.weight, std=0.02 / math.sqrt(2 * config.n_layer))

    def forward(self, x, past_kv=None):
        B, T, C = x.size()

        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.d_model, dim=2)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        if past_kv is not None:
            k = torch.cat([past_kv[0], k], dim=2)
            v = torch.cat([past_kv[1], v], dim=2)

        y = F.scaled_dot_product_attention(
            q, k, v, is_causal=(past_kv is None),
            dropout_p=self.dropout if self.training else 0.0
        )

        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.c_proj(y), (k, v)


# ==========================================
# BLOCK
# ==========================================
class PEERBlock(nn.Module):
    def __init__(self, config, expert_pool):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.d_model)
        self.ln2 = nn.LayerNorm(config.d_model)
        self.attn = CausalSelfAttention(config)
        self.peer = PEERLayer(config, expert_pool)

    def forward(self, x, past_kv=None):
        attn_out, kv = self.attn(self.ln1(x), past_kv)
        x = x + attn_out
        peer_out, aux = self.peer(self.ln2(x))
        x = x + peer_out
        return x, aux, kv


# ==========================================
# MODEL
# ==========================================
class PEERModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.wte = nn.Embedding(config.vocab_size, config.d_model)
        self.wpe = nn.Embedding(config.max_len, config.d_model)
        self.drop = nn.Dropout(config.dropout)

        # Shared bottleneck expert pool
        self.expert_pool = BottleneckExpertPool(config)

        self.blocks = nn.ModuleList([
            PEERBlock(config, self.expert_pool) for _ in range(config.n_layer)
        ])

        self.ln_f = nn.LayerNorm(config.d_model)
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        self.wte.weight = self.lm_head.weight

        self.apply(self._init)
        self._log_params()

    def _init(self, m):
        if isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Embedding):
            if m.num_embeddings in [self.config.vocab_size, self.config.max_len]:
                nn.init.normal_(m.weight, std=0.02)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)

    def _log_params(self):
        total = sum(p.numel() for p in self.parameters())
        expert = (
            self.expert_pool.w_down.weight.numel() +
            self.expert_pool.w_up.weight.numel() +
            self.expert_pool.in_proj.weight.numel() +
            self.expert_pool.out_proj.weight.numel()
        )

        print(f"\n{'='*50}")
        print(f"PARAMETERS")
        print(f"{'='*50}")
        print(f"Total:   {total/1e6:.2f}M")
        print(f"Expert:  {expert/1e6:.2f}M ({100*expert/total:.1f}%)")
        print(f"Dense:   {(total-expert)/1e6:.2f}M")
        print(f"{'='*50}")
        print(f"Expert tables: {config.num_experts:,} Ã— {config.expert_bottleneck} Ã— 2")
        print(f"{'='*50}\n")

    def forward(self, idx, targets=None, past_kv=None):
        b, t = idx.size()

        if past_kv is None:
            pos = torch.arange(t, device=idx.device)
        else:
            pos = torch.arange(past_kv[0][0].size(2), past_kv[0][0].size(2) + t, device=idx.device)
        pos = pos.clamp(max=self.config.max_len - 1)

        x = self.drop(self.wte(idx) + self.wpe(pos))

        total_aux = 0.0
        new_kvs = []
        for i, block in enumerate(self.blocks):
            pv = past_kv[i] if past_kv else None
            x, aux, kv = block(x, pv)
            total_aux += aux
            new_kvs.append(kv)

        logits = self.lm_head(self.ln_f(x))

        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
            loss = loss + total_aux

        return logits, loss, new_kvs

    def get_expert_stats(self):
        return {f"layer_{i}": b.peer.router.get_stats() for i, b in enumerate(self.blocks)}

    def reset_expert_stats(self):
        for b in self.blocks:
            b.peer.router.reset_stats()


# ==========================================
# TRAINING
# ==========================================
def get_lr(step, cfg):
    if step < cfg.warmup_iters:
        return cfg.learning_rate * (step + 1) / cfg.warmup_iters
    if step >= cfg.max_iters:
        return cfg.min_lr
    ratio = (step - cfg.warmup_iters) / (cfg.max_iters - cfg.warmup_iters)
    return cfg.min_lr + 0.5 * (1 + math.cos(math.pi * ratio)) * (cfg.learning_rate - cfg.min_lr)


print("Initializing...")
model = PEERModel(config).to(config.device)

if config.use_8bit_adam:
    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.95), weight_decay=config.weight_decay)
else:
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.95), weight_decay=config.weight_decay)

scaler = torch.amp.GradScaler('cuda') if config.device == 'cuda' else None

step = 0
best_loss = float('inf')
running_loss = 0.0

# ==========================================
# CHECKPOINT
# ==========================================

RESUME_CHECKPOINT = "step_15000.pt"  # â† Change this to your latest checkpoint

resume_path = os.path.join(CHECKPOINT_DIR, RESUME_CHECKPOINT)
if os.path.exists(resume_path):
    print(f"Loading checkpoint: {resume_path}")
    checkpoint = torch.load(resume_path, map_location=config.device)

    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['opt'])
    step = checkpoint['step']
    best_loss = checkpoint.get('loss', float('inf'))

    print(f"âœ“ Resumed from step {step}, best loss {best_loss:.4f}")
else:
    print("No checkpoint found, starting fresh")

print(f"\n{'='*50}")
print("TRAINING")
print(f"{'='*50}\n")

model.train()
start = time.time()

for epoch in range(100):
    for batch in train_loader:
        data = batch["input_ids"].to(config.device)
        x, y = data[:, :-1], data[:, 1:]

        lr = get_lr(step, config)
        for pg in optimizer.param_groups:
            pg['lr'] = lr

        optimizer.zero_grad(set_to_none=True)

        with torch.amp.autocast('cuda', dtype=torch.float16):
            _, loss, _ = model(x, targets=y)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        step += 1

        if step % 50 == 0:
            avg = running_loss / 50
            tps = (step * config.batch_size * config.max_len) / (time.time() - start)
            print(f"Step {step:>5} | Loss {avg:.4f} | LR {lr:.2e} | {tps/1000:.1f}K tok/s")
            if avg < best_loss:
                best_loss = avg
            running_loss = 0.0

        if step % 500 == 0:
            stats = model.get_expert_stats()
            avg_pct = sum(s['pct'] for s in stats.values()) / len(stats)
            print(f"  â””â”€ Expert utilization: {avg_pct:.1f}%")
            model.reset_expert_stats()

        if step % 1000 == 0:
            torch.save({
                'step': step, 'model': model.state_dict(),
                'opt': optimizer.state_dict(), 'loss': best_loss
            }, os.path.join(CHECKPOINT_DIR, f"step_{step}.pt"))
            print(f"  ðŸ’¾ Saved checkpoint")

        if step >= config.max_iters:
            break
    if step >= config.max_iters:
        break

print(f"\n{'='*50}")
print(f"DONE | Steps: {step} | Best: {best_loss:.4f} | Time: {(time.time()-start)/60:.1f}m")
print(f"{'='*50}")

# ==========================================
# GENERATION
# ==========================================
@torch.no_grad()
def generate(prompt, max_new=100, temp=0.8, top_p=0.95):
    model.eval()
    idx = tokenizer.encode(prompt, return_tensors="pt").to(config.device)
    print(f"\n{prompt}", end="")

    past = None
    for _ in range(max_new):
        inp = idx[:, -1:] if past else idx
        if past and past[0][0].size(2) + 1 > config.max_len:
            inp, past = idx[:, -config.max_len:], None

        logits, _, past = model(inp, past_kv=past)
        logits = logits[:, -1] / temp

        sorted_l, sorted_i = torch.sort(logits, descending=True)
        cumsum = torch.cumsum(F.softmax(sorted_l, dim=-1), dim=-1)
        mask = cumsum > top_p
        mask[..., 1:] = mask[..., :-1].clone()
        mask[..., 0] = False
        sorted_l[mask] = float('-inf')

        tok = sorted_i.gather(-1, torch.multinomial(F.softmax(sorted_l, dim=-1), 1))
        print(tokenizer.decode(tok[0]), end="", flush=True)
        idx = torch.cat([idx, tok], dim=1)

        if tok.item() == tokenizer.eos_token_id:
            break
    print()
    model.train()

generate("Once upon a time", 100)
generate("The little rabbit", 80)
